{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOWiR3kGFP/Ds0nh3Z5g2m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/before-born/Neuron/blob/main/imagegenerationusingGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8C4sNgoWTvp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Define the Generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128 * 7 * 7, input_dim=100))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(1, kernel_size=3, activation='sigmoid', padding='same'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(28, 28, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
        "    return model\n",
        "\n",
        "# Define the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Create and compile models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10000\n",
        "batch_size = 64\n",
        "half_batch = batch_size // 2\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Train Discriminator\n",
        "    idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "    real_imgs = x_train[idx]\n",
        "    real_labels = np.ones((half_batch, 1))\n",
        "\n",
        "    noise = np.random.randn(half_batch, 100)\n",
        "    fake_imgs = generator.predict(noise)\n",
        "    fake_labels = np.zeros((half_batch, 1))\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train Generator\n",
        "    noise = np.random.randn(batch_size, 100)\n",
        "    valid_labels = np.ones((batch_size, 1))\n",
        "    g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "        # Save generated images\n",
        "        if epoch % 1000 == 0:\n",
        "            generated_images = generator.predict(np.random.randn(25, 100))\n",
        "            generated_images = 0.5 * generated_images + 0.5  # Rescale images to [0, 1]\n",
        "            fig, axs = plt.subplots(5, 5)\n",
        "            cnt = 0\n",
        "            for i in range(5):\n",
        "                for j in range(5):\n",
        "                    axs[i,j].imshow(generated_images[cnt, :, :, 0], cmap='gray')\n",
        "                    axs[i,j].axis('off')\n",
        "                    cnt += 1\n",
        "            plt.show()\n"
      ]
    }
  ]
}